# FPGA-based-Transformer-implementation
Speech processing systems are widely used in daily life. Intelligent speech assistants improve convenience and efficiency by enabling voice-controlled operations. Transformer, one of the most advanced neural network architectures, has been extensively applied to speech processing tasks. For instance, the Hidden Unit Bidirectional Encoder Representation from Transformer (HuBERT) model has demonstrated strong performance across various speech recognition tasks, requiring only fine-tuning to achieve high accuracy, thereby significantly enhancing the flexibility of speech processing applications.
However, due to its complex network architecture, HuBERT demands substantial computational resources. To meet real-time processing requirements, it is typically deployed on high-performance GPUs for training and inference. Yet, on edge devices, the power consumption of GPU is a major limitation, negatively affecting battery life. In scenarios such as intelligent voice assistants, where the system needs to detect a specific keyword to wake up, continuous listening for potential user commands is essential. Consequently, the high power consumption of GPUs becomes a significant issue.
A common approach to mitigate this problem is offloading speech processing to cloud servers, thereby avoiding on-device GPU usage. However, this raises concerns regarding user privacy and imposes strict requirements on network speed and stability. 
Compared to GPUs, FPGAs offer significantly lower power consumption while enabling high computing speed. Therefore, FPGA deployment can address both the power constraints of edge devices and their reliance on stable network connectivity.
This study proposes deploying HuBERT on FPGAs instead of GPUs and applying model compression techniques such as quantization and knowledge distillation to reduce model size, thereby improving its feasibility for FPGA deployment.
